{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "resume_title.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnwBfS706XDu",
        "outputId": "3369a81c-4cc5-486f-dde8-99777dc764ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.19.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.8 MB 8.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.19.6\n"
          ]
        }
      ],
      "source": [
        "pip install PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWKo6Suw6Ylm",
        "outputId": "fb89ae0d-e29e-42ea-f967-50f396eab547"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "import re\n",
        "import json\n",
        "import pickle\n",
        "import os\n",
        "import sys,fitz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LG7-O3y6Z_0",
        "outputId": "d0cb1c06-cf33-4307-9c8f-b7f25ead6bea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(resume_text):\n",
        "    stopwords_set = set(stopwords.words('english')+['``',\"''\"])\n",
        "    resume_text = re.sub('http\\S+\\s*', ' ', resume_text)  # remove URLs\n",
        "    resume_text = re.sub('RT|cc', ' ', resume_text)  # remove RT and cc\n",
        "    resume_text = re.sub('#\\S+', '', resume_text)  # remove hashtags\n",
        "    resume_text = re.sub('@\\S+', '  ', resume_text)  # remove mentions\n",
        "    resume_text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', resume_text)  # remove punctuations\n",
        "    resume_text = re.sub(r'[^\\x00-\\x7f]',r' ', resume_text) \n",
        "    resume_text = re.sub('\\s+', ' ', resume_text)  # remove extra whitespace\n",
        "    resume_text = resume_text.lower()  # convert to lowercase\n",
        "    resume_text_tokens = word_tokenize(resume_text)  # tokenize\n",
        "    filtered_text = [w for w in resume_text_tokens if not w in stopwords_set]  # remove stopwords\n",
        "    return ' '.join(filtered_text)"
      ],
      "metadata": {
        "id": "1qhTKgFP6im_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_title(text):\n",
        "  with open('/content/gdrive/MyDrive/LSTM MODEL/feature_tokenizer.pickle', 'rb') as handle:\n",
        "    feature_tokenizer = pickle.load(handle)         \n",
        "         \n",
        "  with open('/content/gdrive/MyDrive/LSTM MODEL/dictionary.pickle', 'rb') as handle:\n",
        "      encoding_to_label = pickle.load(handle)\n",
        "\n",
        "  with open(\"/content/gdrive/MyDrive/LSTM MODEL/labels.json\", \"r\") as read_file:\n",
        "              original_labels = json.load(read_file)\n",
        "  \n",
        "  \n",
        "\n",
        "  sen=clean_text(text)\n",
        "  text=sen\n",
        "  \n",
        "\n",
        "  max_length = 500\n",
        "  trunc_type = 'post'\n",
        "  padding_type = 'post'\n",
        "\n",
        "  predict_sequences = feature_tokenizer.texts_to_sequences([text])\n",
        "  predict_padded = pad_sequences(predict_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "  predict_padded = np.array(predict_padded)\n",
        "\n",
        "  model = keras.models.load_model('/content/gdrive/MyDrive/LSTM MODEL/LSTM_model')\n",
        "  prediction = model.predict(predict_padded)\n",
        "\n",
        "  encodings = np.argpartition(prediction[0], -8)[-8:]\n",
        "  encodings = encodings[np.argsort(prediction[0][encodings])]\n",
        "  encodings = reversed(encodings)\n",
        "\n",
        "  data = {}\n",
        "\n",
        "  for encoding in encodings:\n",
        "    label = encoding_to_label[encoding]\n",
        "    probability = prediction[0][encoding] * 100\n",
        "    probability = round(probability, 2)\n",
        "    data[original_labels[label]]=probability\n",
        "\n",
        "  if '.NET Developer' in data.keys():\n",
        "    del(data['.NET Developer'])\n",
        "\n",
        "  print(data)\n",
        "\n",
        "  titles=[[],[]]\n",
        "  for key,values in data.items():\n",
        "    titles[0].append(key)\n",
        "    titles[1].append(values)\n",
        "  \n",
        "  new_data={}\n",
        "  new_data[titles[0][0]]=titles[1][0]\n",
        "  new_data[titles[0][1]]=titles[1][1]\n",
        "\n",
        "  return new_data\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "plA6g0LW6bQe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/Zohaib Khan - CV for Research Assistant - Lab Instructor.txt\",'r',encoding='utf-8',errors='ignore') as f1:\n",
        "  sentence2=f1.read()\n",
        "\n",
        "test=get_title(sentence2)\n",
        "print(test)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-yHrCPQ7M89",
        "outputId": "9ab17ae4-d613-4aab-b3ff-86f58315d4e8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='embedding_1_input'), name='embedding_1_input', description=\"created by layer 'embedding_1_input'\"), but it was called on an input with incompatible shape (None, 500).\n",
            "{'Data Scientist': 24.13, 'Automation Tester': 20.36, 'Advocate': 11.46, 'Health and Fitness': 8.27, 'SAP Developer': 7.7, 'Mechanical Engineer': 7.18, 'Business Analyst': 5.35, 'Blockchain Developer': 3.0}\n",
            "{'Data Scientist': 24.13, 'Automation Tester': 20.36}\n"
          ]
        }
      ]
    }
  ]
}